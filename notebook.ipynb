{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anastasia/Development/TOOLS/anaconda3/lib/python3.5/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf8 -*-\n",
    "import re\n",
    "import sys\n",
    "import argparse\n",
    "import pymorphy2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.cross_validation import StratifiedKFold, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import linear_model, metrics\n",
    "from scipy import interp\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "data_file = \"data/train.txt\"\n",
    "vocab_file = \"data/vocab.txt\"\n",
    "stopwords_file = \"data/stopwords.txt\"\n",
    "win_size = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = []\n",
    "word_inds = {}\n",
    "with open(vocab_file, \"r\") as f:\n",
    "    s = f.read()\n",
    "    vocab = s.split(',')\n",
    "    \n",
    "for i, w in enumerate(vocab):\n",
    "    word_inds[w] = i\n",
    "stopwords = []\n",
    "with open(stopwords_file, \"r\") as f:\n",
    "    s = f.read()\n",
    "    stopwords = s.split(',')\n",
    "brands = [\"mercedes\", \"bmw\", \"mitsubishi\",\"тойота\",\"hyundai\",\"ford\",\"nissan\",\"audi\",\"lexus\",\n",
    "          \"chevrolet\",\"opel\",\"peugeot\",\"mazda\",\"лексус\",\"volvo\", \"мазд\", \"мицубись\", \"ситройня\", \n",
    "          \"киа\", \"renault\",\"kia\", \"toyota\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def f(word):\n",
    "    x = morph.parse(word)[0].normal_form\n",
    "    if re.match(\"[0-9]+\", x):\n",
    "        return \"_number\"\n",
    "    elif x in brands:\n",
    "        return \"_brand\"\n",
    "    else: return x\n",
    "\n",
    "def process_table(table):\n",
    "    data = table[1].str.split(',', 2).apply(pd.Series, 1)\n",
    "    data = data[data[2] != \"\"]\n",
    "    data[2] = data[2].apply(lambda x: str(x)[9:-3])\n",
    "    data.columns = [\"id\", \"label\", \"text\"]\n",
    "    data['label'] = data['label'].apply(lambda x: '1' if x == '3' or x == '4' else '0')\n",
    "    return data\n",
    "\n",
    "def process_text(text):\n",
    "    sample = re.split('\\W+', re.sub('\\W(Н|н)(е|Е) ', ' не', text))\n",
    "    l = [f(x) for x in sample if f(x) not in stopwords]\n",
    "    text = ' '.join(l)\n",
    "    return text\n",
    "\n",
    "def process(path):\n",
    "    data = pd.read_csv(path, delimiter='autoru-', header = None, quoting=3, engine='python')\n",
    "    data = process_table(data)\n",
    "    data['text'] = data['text'].apply(lambda x: process_text(x))\n",
    "    data.to_csv(path + \".proc.txt\", sep='\\t', encoding='utf-8')\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_chunk(f, delimiter):\n",
    "    buf = \"\"\n",
    "    while True:\n",
    "        while delimiter in buf:\n",
    "            pos = buf.index(delimiter)\n",
    "            yield buf[:pos]\n",
    "            buf = buf[pos + len(delimiter):]\n",
    "        chunk = f.read(2048)\n",
    "        if not chunk:\n",
    "            yield buf\n",
    "            break\n",
    "        buf += chunk\n",
    "\n",
    "\n",
    "def get_data(input, flag):\n",
    "    data, labels = [], []\n",
    "    max_size = 0\n",
    "    if flag:\n",
    "        train_data = process(input)\n",
    "        data, labels = train_data['text'], train_data['label']\n",
    "        \n",
    "    else:\n",
    "        with open(input) as f:\n",
    "            gen = read_chunk(f, \"\\n\")\n",
    "            for i in range(80000):\n",
    "                s = next(gen).split('\\t')\n",
    "                data.append(s[-1])\n",
    "                max_size = max(max_size, len(s[-1].split()))\n",
    "                labels.append(s[-2])\n",
    "    return np.array(data), np.array(labels).astype(int), max_size\n",
    "\n",
    "data, labels, max_size = get_data(data_file, False)\n",
    "inv_labels = np.logical_not(labels)\n",
    "max_size += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_frequencies(some_data, out_file):\n",
    "    word_freq = {}\n",
    "    vectorizer = CountVectorizer(vocabulary=vocab)\n",
    "    data_features = vectorizer.fit_transform(some_data).toarray()\n",
    "    words = vectorizer.get_feature_names()\n",
    "    frequencies = np.sum(data_features, axis=0)\n",
    "    #with open(out_file, \"w+\") as f:\n",
    "    for i, (fr, word) in enumerate(sorted(zip(frequencies, words), reverse=True)):\n",
    "        if word not in stopwords:\n",
    "            #f.write(str(fr) + ' ' + word + '\\n')\n",
    "            word_freq[word] = i + 1\n",
    "    return word_freq\n",
    "\n",
    "#good = get_word_frequencies(data[labels], \"data/clean_good_vocab.txt\")\n",
    "#bad = get_word_frequencies(data[inv_labels], \"data/clean_bad_vocab.txt\")\n",
    "\n",
    "def preprocess(data):\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[0])):\n",
    "            if data[i][j] != 0:\n",
    "                val = (bad[vocab[j]] if vocab[j] in bad else len(bad) / (good[vocab[j]] if vocab[j] in good else len(good)))\n",
    "                data[i][j] = data[i][j] * val\n",
    "            if not np.isfinite(data[i][j]):\n",
    "                print(vocab[j])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def contextwin(l, w_size, max_size):\n",
    "    assert (w_size % 2) == 1\n",
    "    assert w_size >= 1\n",
    "    l = list(l)\n",
    "    lpadded = (w_size // 2 * [-1]) + l + (max_size - len(l)) * [-1] + (w_size // 2 * [-1])\n",
    "    out = [lpadded[i:(i + w_size)] for i in range(max_size)]\n",
    "    return np.array(out).astype(int)\n",
    "\n",
    "def text_to_win(data, w_size, max_size):\n",
    "    inds = []\n",
    "    max_len = 0\n",
    "    for d in data:\n",
    "        processed = [word_inds[w] + 1 if w in vocab else 0 for w in d.split()]\n",
    "        processed += (max_size - len(processed)) * [-1]\n",
    "        #x = contextwin(processed, w_size, max_size)\n",
    "        #inds.append(np.array([contextwin(processed, w_size, max_size)]))\n",
    "        inds.append(np.array(processed))\n",
    "    return inds\n",
    "processed_data = text_to_win(data, win_size, max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.array(processed_data)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_mlp(input_var=None):\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, max_size),input_var=input_var)\n",
    "    l_in_drop = lasagne.layers.DropoutLayer(l_in, p=0.2)\n",
    "    \n",
    "    l_hid1 = lasagne.layers.DenseLayer(l_in_drop, num_units=200,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        W=lasagne.init.GlorotUniform())\n",
    "    l_hid1_drop = lasagne.layers.DropoutLayer(l_hid1, p=0.2)   \n",
    "    \n",
    "    l_hid3 = lasagne.layers.DenseLayer(l_hid1_drop, num_units=1200,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    l_hid3_drop = lasagne.layers.DropoutLayer(l_hid3, p=0.5)#0.5\n",
    "    \n",
    "    #l_hid4 = lasagne.layers.DenseLayer(l_hid3_drop, num_units=40,\n",
    "    #        nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    #l_hid4_drop = lasagne.layers.DropoutLayer(l_hid4, p=0.3)\n",
    "    \n",
    "    l_out = lasagne.layers.DenseLayer(l_hid3_drop, num_units=2,\n",
    "        nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    return l_out\n",
    "\n",
    "def build_cnn(input_var=None):\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, 1, max_size, win_size),input_var=input_var)\n",
    "    l_conv1 = lasagne.layers.Conv2DLayer(\n",
    "        l_in, num_filters=32, filter_size=(5, 5),\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        W=lasagne.init.GlorotUniform())\n",
    "    \n",
    "    l_pool1 = lasagne.layers.MaxPool2DLayer(l_conv1, pool_size=(2, 2))\n",
    "    l_conv2 = lasagne.layers.Conv2DLayer(\n",
    "            l_pool1, num_filters=32, filter_size=(2, 2),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    l_pool2 = lasagne.layers.MaxPool2DLayer(l_conv2, pool_size=(2, 2))\n",
    "\n",
    "    l_dense1 = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(l_pool1, p=.5),\n",
    "            num_units=256,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(l_dense1, p=.5),\n",
    "            num_units=2,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    return l_out\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def with_NN(X_train, y_train, X_test, y_test):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3)\n",
    "    # Prepare Theano variables for inputs and targets\n",
    "    input_var = T.matrix('inputs')\n",
    "    target_var = T.ivector('targets')\n",
    "    # Create neural network model\n",
    "    network = build_mlp(input_var)\n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    updates = lasagne.updates.nesterov_momentum(\n",
    "        loss, params, learning_rate=0.01, momentum=0.9)\n",
    "    \n",
    "    #validation\n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                            target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                  dtype=theano.config.floatX)\n",
    "\n",
    "    # theano functions\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "    test_fn = theano.function([input_var, target_var], test_prediction, on_unused_input='warn')\n",
    "    num_epochs = 20\n",
    "    # training\n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        print (epoch)\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        for batch in iterate_minibatches(X_train, y_train, 500, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_val, y_val, 500, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"  training loss:{:.6f},    validation loss:{:.6f}\".format(\n",
    "                train_err / train_batches, val_err / val_batches))\n",
    "        print(\"{},  validation accuracy:\\t\\t{:.2f} %\".format(epoch + 1, val_acc / val_batches * 100))\n",
    "    \n",
    "    test_err, test_acc, test_batches = 0, 0, 0\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    dtrain_predictions, dtrain_predprob, dtrain_targets = [], [], []\n",
    "    \n",
    "    for batch in iterate_minibatches(X_test, y_test, 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        pred = test_fn(inputs, targets)\n",
    "        preds = [1 if fst < 0.5 else 0 for (fst, snd) in pred]\n",
    "        dtrain_predictions.extend(preds)\n",
    "        dtrain_predprob.extend([snd for (fst, snd) in pred])\n",
    "        dtrain_targets.extend(targets)\n",
    "        for value, prediction in zip(targets, preds):\n",
    "            if (prediction and value):\n",
    "                tp += 1\n",
    "            if (prediction and not value):\n",
    "                fp += 1\n",
    "            if (not prediction and value):\n",
    "                fn += 1\n",
    "            if (not prediction and not value):\n",
    "                tn += 1\n",
    "          \n",
    "        test_err += err\n",
    "        test_acc += acc\n",
    "        test_batches += 1\n",
    "    print(\"NN:\")\n",
    "    print ('TP: {0}, TN: {1}, FP: {2}, FN: {3}'.format(tp, tn, fp, fn))\n",
    "    print (\"Precision Score : %f\" % metrics.precision_score(targets, preds))\n",
    "    print (\"Recall Score : %f\" % metrics.recall_score(targets, preds))\n",
    "    print (\"Accuracy : %.4g\" % metrics.accuracy_score(targets, preds)) \n",
    "    print (\"AUC : %f\" % metrics.roc_auc_score(targets, preds))\n",
    "    #print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "    #print(\"  test accuracy:\\t\\t{:.2f} %\".format(test_acc / test_batches * 100))\n",
    "    return dtrain_predictions, dtrain_predprob, dtrain_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def with_all(d_train, l_train, d_test, l_test):\n",
    "    #clf1 = RandomForestClassifier(n_estimators=20)\n",
    "    skf = list(StratifiedKFold(l_train, 5))\n",
    "\n",
    "    clfs = [RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "            RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "            ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "            ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "            GradientBoostingClassifier(subsample=0.5, max_depth=6, n_estimators=200)]\n",
    "    \n",
    "    dataset_blend_train = np.zeros((d_train.shape[0], len(clfs)))\n",
    "    dataset_blend_test = np.zeros((d_test.shape[0], len(clfs)))\n",
    "    \n",
    "    for j, clf in enumerate(clfs):\n",
    "        print (clf)\n",
    "        dataset_blend_test_j = np.zeros((d_test.shape[0], len(skf)))\n",
    "        for i, (train, test) in enumerate(skf):\n",
    "            X_train = d_train[train]\n",
    "            y_train = l_train[train]\n",
    "            X_test = d_train[test]\n",
    "            y_test = l_train[test]\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_submission = clf.predict_proba(X_test)[:,1]\n",
    "            dataset_blend_train[test, j] = y_submission\n",
    "            dataset_blend_test_j[:, i] = clf.predict_proba(d_test)[:,1]\n",
    "        dataset_blend_test[:,j] = dataset_blend_test_j.mean(1)\n",
    "\n",
    "    clf = LogisticRegression()\n",
    "    print(\"штука живи\")\n",
    "    clf.fit(dataset_blend_train, l_train)\n",
    "    print(\"штука жива\")\n",
    "    dtrain_predprob = clf.predict_proba(dataset_blend_test)[:, 1]\n",
    "    dtrain_predictions = [1 if dtrain_predprob[i] > 0.42 else 0 for i in range(len(dtrain_predprob))]\n",
    "    check = zip(l_test, dtrain_predictions)\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    for value, prediction in check:\n",
    "        if (prediction and value):\n",
    "            tp += 1\n",
    "        if (prediction and not value):\n",
    "            fp += 1 \n",
    "        if (not prediction and value):\n",
    "            fn += 1\n",
    "        if (not prediction and not value):\n",
    "            tn += 1\n",
    "    print ('All: TP: {0}, TN: {1}, FP: {2}, FN: {3}'.format(tp, tn, fp, fn))\n",
    "    print (\"Accuracy : %.4g\" % metrics.accuracy_score(l_test, dtrain_predictions))\n",
    "    print (\"AUC : %f\" % metrics.roc_auc_score(l_test, dtrain_predprob))\n",
    "    print (\"Precision : %f\" % metrics.precision_score(l_test, dtrain_predictions))\n",
    "    print (\"Recall : %f\" % metrics.recall_score(l_test, dtrain_predictions))\n",
    "    return dtrain_predictions, dtrain_predprob\n",
    "\n",
    "def with_KNN(X_train, y_train, X_test, y_test):\n",
    "    X, X1, y, y1 = train_test_split(X_train, y_train, test_size=0.5)\n",
    "    clf1 = RandomForestClassifier(n_estimators=40)\n",
    "    clf2 = KNeighborsClassifier()\n",
    "    enc = OneHotEncoder()\n",
    "    clf1.fit(X, y)\n",
    "    print(\"1\")\n",
    "    enc.fit(clf1.apply(X))\n",
    "    print(\"2\")\n",
    "    clf2.fit(enc.transform(clf1.apply(X1)), y1)\n",
    "    print(\"3\")\n",
    "    res = clf2.predict(enc.transform(clf1.apply(X_test))) \n",
    "    res_p = clf2.predict_proba(enc.transform(clf1.apply(X_test)))[:, 1]\n",
    "    print(\"4\")\n",
    "    #feat_imp = sorted(zip(clf2.feature_importances_, range(len(vocab))), reverse=True)\n",
    "    check = zip(y_test, res)\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    for value, prediction in check:\n",
    "        if (prediction and value):\n",
    "            tp += 1\n",
    "        if (prediction and not value):\n",
    "            fp += 1 \n",
    "        if (not prediction and value):\n",
    "            fn += 1\n",
    "        if (not prediction and not value):\n",
    "            tn += 1\n",
    "    print (' KNN: TP: {0}, TN: {1}, FP: {2}, FN: {3}'.format(tp, tn, fp, fn))\n",
    "    print (\"Precision Score : %f\" % metrics.precision_score(y_test, res))\n",
    "    print (\"Recall Score : %f\" % metrics.recall_score(y_test, res))\n",
    "    print (\"Accuracy : %.4g\" % metrics.accuracy_score(y_test, res))\n",
    "    return res, res_p\n",
    "        \n",
    "def with_XGB(X_train, y_train, X_test, y_test):\n",
    "    clf = XGBClassifier(learning_rate =0.03, n_estimators=200, max_depth=6,\n",
    "                        min_child_weight=4, gamma=0.1, subsample=0.8, \n",
    "                        colsample_bytree=0.8, objective= 'binary:logistic', \n",
    "                        nthread=4, scale_pos_weight=1, seed=27)\n",
    "    clf.fit(X_train, y_train)\n",
    "    res = clf.predict(X_test)\n",
    "    res_p = clf.predict_proba(X_test)[:, 1]\n",
    "    print(\"1\")\n",
    "    feat_imp = pd.Series(clf.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_ind = [ vocab[int(f[1:])] for f in feat_imp.index]\n",
    "    words_imp = zip(feat_imp.values, feat_ind)\n",
    "    #print((list)words_imp[:10])\n",
    "    check = zip(y_test, res)\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    for value, prediction in check:\n",
    "        if (prediction and value):\n",
    "            tp += 1\n",
    "        if (prediction and not value):\n",
    "            fp += 1 \n",
    "        if (not prediction and value):\n",
    "            fn += 1\n",
    "        if (not prediction and not value):\n",
    "            tn += 1\n",
    "    print ('XGB: TP: {0}, TN: {1}, FP: {2}, FN: {3}'.format(tp, tn, fp, fn))\n",
    "    print (\"Precision Score : %f\" % metrics.precision_score(y_test, res))\n",
    "    print (\"Recall Score : %f\" % metrics.recall_score(y_test, res))\n",
    "    print (\"Accuracy : %.4g\" % metrics.accuracy_score(y_test, res))\n",
    "    #feat_imp = sorted(zip(clf2.feature_importances_, range(len(vocab))), reverse=True)\n",
    "    return res, res_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test(with_probas=True):\n",
    "    cv = StratifiedKFold(labels, n_folds=5)\n",
    "    for i, (train, test) in enumerate(cv):\n",
    "        vectorizer = CountVectorizer(vocabulary=vocab)\n",
    "        features = vectorizer.fit_transform(data[train])\n",
    "\n",
    "        # preprocessing with tfidf\n",
    "        #transformer = TfidfTransformer()\n",
    "        #tfidf_features = transformer.fit(features).transform(features)\n",
    "        #X = tfidf_features.toarray()\n",
    "\n",
    "        # preprocessing with bad/good ranking\n",
    "        #X_train = preprocess(features.toarray())\n",
    "\n",
    "        X_train, y_train = features.toarray(), labels[train]\n",
    "        X_test, y_test = vectorizer.transform(data[test]).toarray(), labels[test]\n",
    "        #X_test = preprocess(X_test.toarray())\n",
    "\n",
    "        \n",
    "        res, res_p = with_XGB(X_train, y_train, X_test, y_test)\n",
    "        fpr, tpr, _ = roc_curve(y_test, res_p)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=1, label='XGB (area = %0.2f)' % (roc_auc))  \n",
    "        \n",
    "        res, res_p = with_all(X_train, y_train, X_test, y_test)\n",
    "        fpr, tpr, _ = roc_curve(y_test, res_p)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=1, label='Ensemble (area = %0.2f)' % (roc_auc))            \n",
    "          \n",
    "\n",
    "        res, res_p = with_KNN(X_train, y_train, X_test, y_test)\n",
    "        fpr, tpr, _ = roc_curve(y_test, res_p)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=1, label='KNN+RF (area = %0.2f)' % (roc_auc))\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Luck')\n",
    "        plt.xlim([-0.05, 1.05])\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver operating characteristic example')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "  training loss:nan,    validation loss:nan\n",
      "1,  validation accuracy:\t\t93.40 %\n",
      "1\n",
      "  training loss:nan,    validation loss:nan\n",
      "2,  validation accuracy:\t\t93.40 %\n",
      "2\n",
      "  training loss:nan,    validation loss:nan\n",
      "3,  validation accuracy:\t\t93.40 %\n",
      "3\n",
      "  training loss:nan,    validation loss:nan\n",
      "4,  validation accuracy:\t\t93.40 %\n",
      "4\n",
      "  training loss:nan,    validation loss:nan\n",
      "5,  validation accuracy:\t\t93.40 %\n",
      "5\n",
      "  training loss:nan,    validation loss:nan\n",
      "6,  validation accuracy:\t\t93.40 %\n",
      "6\n",
      "  training loss:nan,    validation loss:nan\n",
      "7,  validation accuracy:\t\t93.40 %\n",
      "7\n",
      "  training loss:nan,    validation loss:nan\n",
      "8,  validation accuracy:\t\t93.40 %\n",
      "8\n",
      "  training loss:nan,    validation loss:nan\n",
      "9,  validation accuracy:\t\t93.40 %\n",
      "9\n",
      "  training loss:nan,    validation loss:nan\n",
      "10,  validation accuracy:\t\t93.40 %\n",
      "10\n",
      "  training loss:nan,    validation loss:nan\n",
      "11,  validation accuracy:\t\t93.40 %\n",
      "11\n",
      "  training loss:nan,    validation loss:nan\n",
      "12,  validation accuracy:\t\t93.40 %\n",
      "12\n",
      "  training loss:nan,    validation loss:nan\n",
      "13,  validation accuracy:\t\t93.40 %\n",
      "13\n",
      "  training loss:nan,    validation loss:nan\n",
      "14,  validation accuracy:\t\t93.40 %\n",
      "14\n",
      "  training loss:nan,    validation loss:nan\n",
      "15,  validation accuracy:\t\t93.40 %\n",
      "15\n",
      "  training loss:nan,    validation loss:nan\n",
      "16,  validation accuracy:\t\t93.40 %\n",
      "16\n",
      "  training loss:nan,    validation loss:nan\n",
      "17,  validation accuracy:\t\t93.40 %\n",
      "17\n",
      "  training loss:nan,    validation loss:nan\n",
      "18,  validation accuracy:\t\t93.40 %\n",
      "18\n",
      "  training loss:nan,    validation loss:nan\n",
      "19,  validation accuracy:\t\t93.40 %\n",
      "19\n",
      "  training loss:nan,    validation loss:nan\n",
      "20,  validation accuracy:\t\t93.40 %\n",
      "NN:\n",
      "TP: 0, TN: 7479, FP: 0, FN: 521\n",
      "Precision Score : 0.000000\n",
      "Recall Score : 0.000000\n",
      "Accuracy : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anastasia/Development/TOOLS/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:26: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 1 is not part of the computational graph needed to compute the outputs: targets.\n",
      "To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only one class present in y_true. ROC AUC score is not defined in that case.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-64589d7327a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mtestNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-64589d7327a6>\u001b[0m in \u001b[0;36mtestNN\u001b[0;34m(with_probas)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#X_test = preprocess(X_test.toarray())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwith_NN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-b726cb039d75>\u001b[0m in \u001b[0;36mwith_NN\u001b[0;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Recall Score : %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy : %.4g\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"AUC : %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0;31m#print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;31m#print(\"  test accuracy:\\t\\t{:.2f} %\".format(test_acc / test_batches * 100))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anastasia/Development/TOOLS/anaconda3/lib/python3.5/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m    251\u001b[0m     return _average_binary_score(\n\u001b[1;32m    252\u001b[0m         \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anastasia/Development/TOOLS/anaconda3/lib/python3.5/site-packages/sklearn/metrics/base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/anastasia/Development/TOOLS/anaconda3/lib/python3.5/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             raise ValueError(\"Only one class present in y_true. ROC AUC score \"\n\u001b[0m\u001b[1;32m    245\u001b[0m                              \"is not defined in that case.\")\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Only one class present in y_true. ROC AUC score is not defined in that case."
     ]
    }
   ],
   "source": [
    "def testNN(with_probas=True):\n",
    "    cv = StratifiedKFold(labels, n_folds=5)\n",
    "    for i, (train, test) in enumerate(cv):\n",
    "        vectorizer = CountVectorizer(vocabulary=vocab)\n",
    "        #features = vectorizer.fit_transform(data[train])\n",
    "        \n",
    "        # preprocessing with tfidf\n",
    "        #transformer = TfidfTransformer()\n",
    "        #tfidf_features = transformer.fit(features).transform(features)\n",
    "        #X = tfidf_features.toarray()\n",
    "        \n",
    "        # preprocessing with bad/good ranking\n",
    "        #X_train = preprocess(features.toarray())\n",
    "        \n",
    "        X_train, y_train = np.array(processed_data)[train], np.array(labels[train]).astype(np.int32)\n",
    "        X_test, y_test = np.array(processed_data)[test], np.array(labels[test]).astype(np.int32)\n",
    "        #X_test = preprocess(X_test.toarray())\n",
    "        \n",
    "        res, res_p, l = with_NN(X_train, y_train, X_test, y_test)\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(l, res_p)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=1, label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "        check = zip(y_test, res)\n",
    "        tp, tn, fp, fn = 0, 0, 0, 0\n",
    "        for value, prediction in check:\n",
    "            if (prediction and value):\n",
    "                tp += 1\n",
    "            if (prediction and not value):\n",
    "                fp += 1 \n",
    "            if (not prediction and value):\n",
    "                fn += 1\n",
    "            if (not prediction and not value):\n",
    "                tn += 1\n",
    "        print ('TP: {0}, TN: {1}, FP: {2}, FN: {3}'.format(tp, tn, fp, fn))\n",
    "        print (\"Precision Score : %f\" % metrics.precision_score(y_test, res))\n",
    "        print (\"Recall Score : %f\" % metrics.recall_score(y_test, res))\n",
    "        print (\"Accuracy : %.4g\" % metrics.accuracy_score(y_test, res))\n",
    "    if with_probas:\n",
    "        plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Luck')\n",
    "        plt.xlim([-0.05, 1.05])\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver operating characteristic example')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "\n",
    "testNN()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
